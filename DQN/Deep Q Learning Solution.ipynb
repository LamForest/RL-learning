{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 grayscale frames of shape 84, 84 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 84, 84, 1]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-49f3f6294684>:33: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From d:\\conda\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\layers\\python\\layers\\layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From d:\\conda\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\layers\\python\\layers\\layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-49f3f6294684>:59: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "WARNING:tensorflow:From d:\\conda\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "[[0.         0.00638199 0.03079277 0.        ]\n",
      " [0.         0.00638199 0.03079277 0.        ]]\n",
      "99.9362\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParametersCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # Make model copier object\n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # For 'system/' summaries, usefull to check if currrent process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        if i % 10000 == 0:\n",
    "            print(\"populating: {} / {}\".format(i, replay_memory_init_size))\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "    print(\"Finish populating replay memory...\")\n",
    "\n",
    "    # Record videos\n",
    "    # Add env Monitor wrapper\n",
    "    env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                estimator_copy.make(sess)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "        \n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint D:\\gao_tianlin\\LamForestGithub\\ml\\reinforcement-learning\\DQN\\experiments\\Breakout-v0\\checkpoints\\model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from D:\\gao_tianlin\\LamForestGithub\\ml\\reinforcement-learning\\DQN\\experiments\\Breakout-v0\\checkpoints\\model\n",
      "Populating replay memory...\n",
      "populating: 0 / 500\n",
      "Finish populating replay memory...\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 269 (269) @ Episode 1/10000, loss: 0.0053129559382796296\n",
      "Episode Reward: 2.0\n",
      "Step 325 (594) @ Episode 2/10000, loss: 0.0378240346908569346\n",
      "Episode Reward: 2.0\n",
      "Step 289 (883) @ Episode 3/10000, loss: 0.0083876848220825235\n",
      "Episode Reward: 2.0\n",
      "Step 231 (1114) @ Episode 4/10000, loss: 0.0081265326589345935\n",
      "Episode Reward: 1.0\n",
      "Step 278 (1392) @ Episode 5/10000, loss: 0.0382956266403198245\n",
      "Episode Reward: 2.0\n",
      "Step 218 (1610) @ Episode 6/10000, loss: 7.858243407099508e-065\n",
      "Episode Reward: 1.0\n",
      "Step 247 (1857) @ Episode 7/10000, loss: 0.00018897684640251185\n",
      "Episode Reward: 2.0\n",
      "Step 274 (2131) @ Episode 8/10000, loss: 1.550234628666658e-055\n",
      "Episode Reward: 2.0\n",
      "Step 245 (2376) @ Episode 9/10000, loss: 0.00135333568323403656\n",
      "Episode Reward: 1.0\n",
      "Step 237 (2613) @ Episode 10/10000, loss: 0.03095652908086776745\n",
      "Episode Reward: 1.0\n",
      "Step 351 (2964) @ Episode 11/10000, loss: 0.00037361355498433113\n",
      "Episode Reward: 3.0\n",
      "Step 290 (3254) @ Episode 12/10000, loss: 4.3696363718481734e-05\n",
      "Episode Reward: 2.0\n",
      "Step 186 (3440) @ Episode 13/10000, loss: 3.186806134181097e-055\n",
      "Episode Reward: 0.0\n",
      "Step 202 (3642) @ Episode 14/10000, loss: 0.00017465463315602392\n",
      "Episode Reward: 1.0\n",
      "Step 176 (3818) @ Episode 15/10000, loss: 4.9274516641162336e-05\n",
      "Episode Reward: 0.0\n",
      "Step 172 (3990) @ Episode 16/10000, loss: 0.00030886795138940215\n",
      "Episode Reward: 0.0\n",
      "Step 262 (4252) @ Episode 17/10000, loss: 3.6963989259675145e-05\n",
      "Episode Reward: 2.0\n",
      "Step 188 (4440) @ Episode 18/10000, loss: 0.00042925946763716645\n",
      "Episode Reward: 0.0\n",
      "Step 297 (4737) @ Episode 19/10000, loss: 0.00089876184938475494\n",
      "Episode Reward: 2.0\n",
      "Step 174 (4911) @ Episode 20/10000, loss: 1.848337342380546e-052\n",
      "Episode Reward: 0.0\n",
      "Step 584 (5495) @ Episode 21/10000, loss: 9.982124174712226e-053\n",
      "Episode Reward: 7.0\n",
      "Step 178 (5673) @ Episode 22/10000, loss: 0.00014086227747611701\n",
      "Episode Reward: 0.0\n",
      "Step 277 (5950) @ Episode 23/10000, loss: 4.104976324015297e-054\n",
      "Episode Reward: 2.0\n",
      "Step 173 (6123) @ Episode 24/10000, loss: 3.468323120614514e-055\n",
      "Episode Reward: 0.0\n",
      "Step 399 (6522) @ Episode 25/10000, loss: 0.00017646924243308604\n",
      "Episode Reward: 4.0\n",
      "Step 268 (6790) @ Episode 26/10000, loss: 0.00013886677334085107\n",
      "Episode Reward: 2.0\n",
      "Step 173 (6963) @ Episode 27/10000, loss: 4.764458572026342e-056\n",
      "Episode Reward: 0.0\n",
      "Step 344 (7307) @ Episode 28/10000, loss: 0.03101976215839386055\n",
      "Episode Reward: 3.0\n",
      "Step 172 (7479) @ Episode 29/10000, loss: 9.887696796795353e-055\n",
      "Episode Reward: 0.0\n",
      "Step 173 (7652) @ Episode 30/10000, loss: 6.536587898153812e-056\n",
      "Episode Reward: 0.0\n",
      "Step 181 (7833) @ Episode 31/10000, loss: 9.592460992280394e-054\n",
      "Episode Reward: 0.0\n",
      "Step 210 (8043) @ Episode 32/10000, loss: 0.00036610080860555175\n",
      "Episode Reward: 1.0\n",
      "Step 206 (8249) @ Episode 33/10000, loss: 3.7456899008248e-05055\n",
      "Episode Reward: 1.0\n",
      "Step 311 (8560) @ Episode 34/10000, loss: 0.00040923751657828697\n",
      "Episode Reward: 2.0\n",
      "Step 180 (8740) @ Episode 35/10000, loss: 9.79831165750511e-0538\n",
      "Episode Reward: 0.0\n",
      "Step 236 (8976) @ Episode 36/10000, loss: 0.00029602082213386893\n",
      "Episode Reward: 1.0\n",
      "Step 172 (9148) @ Episode 37/10000, loss: 4.270049976184964e-055\n",
      "Episode Reward: 0.0\n",
      "Step 336 (9484) @ Episode 38/10000, loss: 3.406128962524235e-055\n",
      "Episode Reward: 3.0\n",
      "Step 208 (9692) @ Episode 39/10000, loss: 0.00055409909691661608\n",
      "Episode Reward: 1.0\n",
      "Step 183 (9875) @ Episode 40/10000, loss: 0.00010660726547939703\n",
      "Episode Reward: 0.0\n",
      "Step 124 (9999) @ Episode 41/10000, loss: 4.1922299715224653e-05\n",
      "Copied model parameters to target network.\n",
      "Step 242 (10117) @ Episode 41/10000, loss: 0.00011297508171992376\n",
      "Episode Reward: 1.0\n",
      "Step 334 (10451) @ Episode 42/10000, loss: 0.00011290019756415859\n",
      "Episode Reward: 3.0\n",
      "Step 183 (10634) @ Episode 43/10000, loss: 0.00011299140896880999\n",
      "Episode Reward: 0.0\n",
      "Step 206 (10840) @ Episode 44/10000, loss: 0.00037671127938665457\n",
      "Episode Reward: 1.0\n",
      "Step 182 (11022) @ Episode 45/10000, loss: 7.83424184191972e-0552\n",
      "Episode Reward: 0.0\n",
      "Step 214 (11236) @ Episode 46/10000, loss: 0.00012247884296812117\n",
      "Episode Reward: 1.0\n",
      "Step 225 (11461) @ Episode 47/10000, loss: 7.013602589722723e-055\n",
      "Episode Reward: 1.0\n",
      "Step 242 (11703) @ Episode 48/10000, loss: 0.03000553138554096213\n",
      "Episode Reward: 1.0\n",
      "Step 312 (12015) @ Episode 49/10000, loss: 0.02829150483012199452\n",
      "Episode Reward: 3.0\n",
      "Step 236 (12251) @ Episode 50/10000, loss: 0.02962092310190200815\n",
      "Episode Reward: 1.0\n",
      "Step 180 (12431) @ Episode 51/10000, loss: 0.03118805214762687714\n",
      "Episode Reward: 0.0\n",
      "Step 234 (12665) @ Episode 52/10000, loss: 0.00015200875350274146\n",
      "Episode Reward: 2.0\n",
      "Step 277 (12942) @ Episode 53/10000, loss: 0.00022583844838663936\n",
      "Episode Reward: 2.0\n",
      "Step 230 (13172) @ Episode 54/10000, loss: 0.00010586855933070183\n",
      "Episode Reward: 1.0\n",
      "Step 356 (13528) @ Episode 55/10000, loss: 0.00033785091363824904\n",
      "Episode Reward: 3.0\n",
      "Step 492 (14020) @ Episode 56/10000, loss: 0.00098511192481964832\n",
      "Episode Reward: 6.0\n",
      "Step 178 (14198) @ Episode 57/10000, loss: 6.905851478222758e-056\n",
      "Episode Reward: 0.0\n",
      "Step 235 (14433) @ Episode 58/10000, loss: 0.00039544492028653622\n",
      "Episode Reward: 1.0\n",
      "Step 175 (14608) @ Episode 59/10000, loss: 0.03071470558643341637\n",
      "Episode Reward: 0.0\n",
      "Step 209 (14817) @ Episode 60/10000, loss: 0.00018636688764672726\n",
      "Episode Reward: 1.0\n",
      "Step 171 (14988) @ Episode 61/10000, loss: 0.00013598633813671768\n",
      "Episode Reward: 0.0\n",
      "Step 247 (15235) @ Episode 62/10000, loss: 0.00023021172091830522\n",
      "Episode Reward: 1.0\n",
      "Step 280 (15515) @ Episode 63/10000, loss: 6.803091673646122e-052\n",
      "Episode Reward: 2.0\n",
      "Step 315 (15830) @ Episode 64/10000, loss: 0.00025324069429188967\n",
      "Episode Reward: 3.0\n",
      "Step 169 (15999) @ Episode 65/10000, loss: 0.00028187566203996548\n",
      "Episode Reward: 0.0\n",
      "Step 267 (16266) @ Episode 66/10000, loss: 0.00011764914961531758\n",
      "Episode Reward: 2.0\n",
      "Step 217 (16483) @ Episode 67/10000, loss: 7.19941162969917e-0553\n",
      "Episode Reward: 1.0\n",
      "Step 344 (16827) @ Episode 68/10000, loss: 0.00014595431275665767\n",
      "Episode Reward: 3.0\n",
      "Step 309 (17136) @ Episode 69/10000, loss: 4.1480943764327094e-05\n",
      "Episode Reward: 3.0\n",
      "Step 232 (17368) @ Episode 70/10000, loss: 0.03106745146214962055\n",
      "Episode Reward: 1.0\n",
      "Step 222 (17590) @ Episode 71/10000, loss: 6.166138336993754e-057\n",
      "Episode Reward: 1.0\n",
      "Step 166 (17756) @ Episode 72/10000, loss: 0.05853801220655441737\n",
      "Episode Reward: 0.0\n",
      "Step 434 (18190) @ Episode 73/10000, loss: 4.798633744940162e-056\n",
      "Episode Reward: 4.0\n",
      "Step 205 (18395) @ Episode 74/10000, loss: 0.00074459990719333297\n",
      "Episode Reward: 0.0\n",
      "Step 236 (18631) @ Episode 75/10000, loss: 6.925728666828945e-058\n",
      "Episode Reward: 1.0\n",
      "Step 164 (18795) @ Episode 76/10000, loss: 0.00186386657878756521\n",
      "Episode Reward: 0.0\n",
      "Step 175 (18970) @ Episode 77/10000, loss: 5.895440699532628e-058\n",
      "Episode Reward: 0.0\n",
      "Step 178 (19148) @ Episode 78/10000, loss: 0.00033364602131769064\n",
      "Episode Reward: 0.0\n",
      "Step 175 (19323) @ Episode 79/10000, loss: 0.02651510573923587886\n",
      "Episode Reward: 0.0\n",
      "Step 343 (19666) @ Episode 80/10000, loss: 7.148790609790012e-058\n",
      "Episode Reward: 3.0\n",
      "Step 180 (19846) @ Episode 81/10000, loss: 9.811860945774242e-058\n",
      "Episode Reward: 0.0\n",
      "Step 153 (19999) @ Episode 82/10000, loss: 0.00012600843911059277\n",
      "Copied model parameters to target network.\n",
      "Step 327 (20173) @ Episode 82/10000, loss: 0.02927283756434917481\n",
      "Episode Reward: 3.0\n",
      "Step 318 (20491) @ Episode 83/10000, loss: 0.00011457204527687281\n",
      "Episode Reward: 3.0\n",
      "Step 246 (20737) @ Episode 84/10000, loss: 0.02558580599725246458\n",
      "Episode Reward: 2.0\n",
      "Step 292 (21029) @ Episode 85/10000, loss: 0.00021916098194196823\n",
      "Episode Reward: 2.0\n",
      "Step 323 (21352) @ Episode 86/10000, loss: 0.00012301509559620172\n",
      "Episode Reward: 3.0\n",
      "Step 164 (21516) @ Episode 87/10000, loss: 8.373135642614216e-054\n",
      "Episode Reward: 0.0\n",
      "Step 340 (21856) @ Episode 88/10000, loss: 0.00106321123894304046\n",
      "Episode Reward: 3.0\n",
      "Step 169 (22025) @ Episode 89/10000, loss: 7.921185169834644e-055\n",
      "Episode Reward: 0.0\n",
      "Step 371 (22396) @ Episode 90/10000, loss: 0.00326691986992955269\n",
      "Episode Reward: 3.0\n",
      "Step 245 (22641) @ Episode 91/10000, loss: 6.468952778959647e-056\n",
      "Episode Reward: 1.0\n",
      "Step 217 (22858) @ Episode 92/10000, loss: 0.00127620284911245196\n",
      "Episode Reward: 1.0\n",
      "Step 214 (23072) @ Episode 93/10000, loss: 0.00253587518818676473\n",
      "Episode Reward: 1.0\n",
      "Step 276 (23348) @ Episode 94/10000, loss: 0.02791656926274299673\n",
      "Episode Reward: 2.0\n",
      "Step 287 (23635) @ Episode 95/10000, loss: 0.02305103093385696433\n",
      "Episode Reward: 3.0\n",
      "Step 372 (24007) @ Episode 96/10000, loss: 0.00024661066709086384\n",
      "Episode Reward: 3.0\n",
      "Step 183 (24190) @ Episode 97/10000, loss: 0.00014166267646942288\n",
      "Episode Reward: 0.0\n",
      "Step 175 (24365) @ Episode 98/10000, loss: 0.00477840891107916826\n",
      "Episode Reward: 0.0\n",
      "Step 525 (24890) @ Episode 99/10000, loss: 0.00059636548394337336\n",
      "Episode Reward: 6.0\n",
      "Step 177 (25067) @ Episode 100/10000, loss: 0.02817561849951744985\n",
      "Episode Reward: 0.0\n",
      "Step 205 (25272) @ Episode 101/10000, loss: 0.00130018428899347788\n",
      "Episode Reward: 1.0\n",
      "Step 348 (25620) @ Episode 102/10000, loss: 0.00088963727466762078\n",
      "Episode Reward: 3.0\n",
      "Step 170 (25790) @ Episode 103/10000, loss: 0.00054452370386570692\n",
      "Episode Reward: 0.0\n",
      "Step 233 (26023) @ Episode 104/10000, loss: 0.00023431121371686459\n",
      "Episode Reward: 1.0\n",
      "Step 171 (26194) @ Episode 105/10000, loss: 0.00011716010340023786\n",
      "Episode Reward: 0.0\n",
      "Step 193 (26387) @ Episode 106/10000, loss: 0.00053378986194729865\n",
      "Episode Reward: 0.0\n",
      "Step 168 (26555) @ Episode 107/10000, loss: 0.00020442571258172393\n",
      "Episode Reward: 0.0\n",
      "Step 397 (26952) @ Episode 108/10000, loss: 0.00056900089839473374\n",
      "Episode Reward: 4.0\n",
      "Step 277 (27229) @ Episode 109/10000, loss: 0.00229262234643101726\n",
      "Episode Reward: 2.0\n",
      "Step 463 (27692) @ Episode 110/10000, loss: 0.00163060997147113087\n",
      "Episode Reward: 5.0\n",
      "Step 234 (27926) @ Episode 111/10000, loss: 0.00910875014960765885\n",
      "Episode Reward: 1.0\n",
      "Step 267 (28193) @ Episode 112/10000, loss: 0.00058256607735529548\n",
      "Episode Reward: 2.0\n",
      "Step 166 (28359) @ Episode 113/10000, loss: 7.150512101361528e-055\n",
      "Episode Reward: 0.0\n",
      "Step 338 (28697) @ Episode 114/10000, loss: 0.00064845592714846136\n",
      "Episode Reward: 3.0\n",
      "Step 252 (28949) @ Episode 115/10000, loss: 0.00111167505383491527\n",
      "Episode Reward: 2.0\n",
      "Step 231 (29180) @ Episode 116/10000, loss: 0.00118182413280010225\n",
      "Episode Reward: 1.0\n",
      "Step 242 (29422) @ Episode 117/10000, loss: 0.00024880957789719105\n",
      "Episode Reward: 1.0\n",
      "Step 168 (29590) @ Episode 118/10000, loss: 0.00028161797672510147\n",
      "Episode Reward: 0.0\n",
      "Step 169 (29759) @ Episode 119/10000, loss: 0.00035813730210065848\n",
      "Episode Reward: 0.0\n",
      "Step 240 (29999) @ Episode 120/10000, loss: 0.00014847048441879458\n",
      "Copied model parameters to target network.\n",
      "Step 276 (30035) @ Episode 120/10000, loss: 0.00033106375485658646\n",
      "Episode Reward: 2.0\n",
      "Step 407 (30442) @ Episode 121/10000, loss: 0.00082514894893392924\n",
      "Episode Reward: 4.0\n",
      "Step 209 (30651) @ Episode 122/10000, loss: 0.01764077320694923484\n",
      "Episode Reward: 1.0\n",
      "Step 302 (30953) @ Episode 123/10000, loss: 0.00067364179994910968\n",
      "Episode Reward: 2.0\n",
      "Step 241 (31194) @ Episode 124/10000, loss: 0.00028940563788637524\n",
      "Episode Reward: 1.0\n",
      "Step 263 (31457) @ Episode 125/10000, loss: 0.00557865388691425382\n",
      "Episode Reward: 2.0\n",
      "Step 281 (31738) @ Episode 126/10000, loss: 0.00078625325113534935\n",
      "Episode Reward: 2.0\n",
      "Step 350 (32088) @ Episode 127/10000, loss: 0.02265674993395805487\n",
      "Episode Reward: 3.0\n",
      "Step 323 (32411) @ Episode 128/10000, loss: 0.00472819712013006285\n",
      "Episode Reward: 3.0\n",
      "Step 337 (32748) @ Episode 129/10000, loss: 0.02795177139341831255\n",
      "Episode Reward: 3.0\n",
      "Step 167 (32915) @ Episode 130/10000, loss: 0.00126914028078317644\n",
      "Episode Reward: 0.0\n",
      "Step 174 (33089) @ Episode 131/10000, loss: 8.162137237377465e-051\n",
      "Episode Reward: 0.0\n",
      "Step 240 (33329) @ Episode 132/10000, loss: 0.00145115796476602554\n",
      "Episode Reward: 1.0\n",
      "Step 239 (33568) @ Episode 133/10000, loss: 0.00461266376078128836\n",
      "Episode Reward: 1.0\n",
      "Step 252 (33820) @ Episode 134/10000, loss: 0.00057658943114802245\n",
      "Episode Reward: 1.0\n",
      "Step 181 (34001) @ Episode 135/10000, loss: 0.00119830155745148664\n",
      "Episode Reward: 0.0\n",
      "Step 183 (34184) @ Episode 136/10000, loss: 0.00010723102604970336\n",
      "Episode Reward: 0.0\n",
      "Step 189 (34373) @ Episode 137/10000, loss: 0.00286161596886813645\n",
      "Episode Reward: 0.0\n",
      "Step 170 (34543) @ Episode 138/10000, loss: 0.01916494220495224276\n",
      "Episode Reward: 0.0\n",
      "Step 208 (34751) @ Episode 139/10000, loss: 0.00014096660015638918\n",
      "Episode Reward: 1.0\n",
      "Step 167 (34918) @ Episode 140/10000, loss: 0.00028092280263081193\n",
      "Episode Reward: 0.0\n",
      "Step 280 (35198) @ Episode 141/10000, loss: 0.00012256539775989956\n",
      "Episode Reward: 2.0\n",
      "Step 279 (35477) @ Episode 142/10000, loss: 0.00106818939093500385\n",
      "Episode Reward: 2.0\n",
      "Step 228 (35705) @ Episode 143/10000, loss: 0.0233466736972332-056\n",
      "Episode Reward: 1.0\n",
      "Step 172 (35877) @ Episode 144/10000, loss: 0.00019367717322893444\n",
      "Episode Reward: 0.0\n",
      "Step 348 (36225) @ Episode 145/10000, loss: 0.00052508106455206873\n",
      "Episode Reward: 3.0\n",
      "Step 186 (36411) @ Episode 146/10000, loss: 0.00024850436602719133\n",
      "Episode Reward: 0.0\n",
      "Step 173 (36584) @ Episode 147/10000, loss: 0.00014222905156202614\n",
      "Episode Reward: 0.0\n",
      "Step 174 (36758) @ Episode 148/10000, loss: 6.290175952017307e-058\n",
      "Episode Reward: 0.0\n",
      "Step 305 (37063) @ Episode 149/10000, loss: 0.00012358168896753344\n",
      "Episode Reward: 2.0\n",
      "Step 268 (37331) @ Episode 150/10000, loss: 0.00010792267858050764\n",
      "Episode Reward: 2.0\n",
      "Step 290 (37621) @ Episode 151/10000, loss: 0.00028732890496030455\n",
      "Episode Reward: 2.0\n",
      "Step 177 (37798) @ Episode 152/10000, loss: 0.00252447463572025355\n",
      "Episode Reward: 0.0\n",
      "Step 347 (38145) @ Episode 153/10000, loss: 0.00303400377742946156\n",
      "Episode Reward: 3.0\n",
      "Step 163 (38308) @ Episode 154/10000, loss: 6.620772182941437e-055\n",
      "Episode Reward: 0.0\n",
      "Step 261 (38569) @ Episode 155/10000, loss: 0.00015824512229301035\n",
      "Episode Reward: 2.0\n",
      "Step 231 (38800) @ Episode 156/10000, loss: 0.00012211140710860495\n",
      "Episode Reward: 1.0\n",
      "Step 239 (39039) @ Episode 157/10000, loss: 0.00013202198897488415\n",
      "Episode Reward: 1.0\n",
      "Step 177 (39216) @ Episode 158/10000, loss: 0.00110226869583129885\n",
      "Episode Reward: 0.0\n",
      "Step 168 (39384) @ Episode 159/10000, loss: 0.00142513262107968338\n",
      "Episode Reward: 0.0\n",
      "Step 236 (39620) @ Episode 160/10000, loss: 0.00164413917809724833\n",
      "Episode Reward: 1.0\n",
      "Step 221 (39841) @ Episode 161/10000, loss: 0.00095909117953851823\n",
      "Episode Reward: 1.0\n",
      "Step 158 (39999) @ Episode 162/10000, loss: 0.00011832905875053257\n",
      "Copied model parameters to target network.\n",
      "Step 168 (40009) @ Episode 162/10000, loss: 0.00072456966154277327\n",
      "Episode Reward: 0.0\n",
      "Step 294 (40303) @ Episode 163/10000, loss: 0.00015873640950303525\n",
      "Episode Reward: 3.0\n",
      "Step 206 (40509) @ Episode 164/10000, loss: 0.00096393149578943852\n",
      "Episode Reward: 0.0\n",
      "Step 362 (40871) @ Episode 165/10000, loss: 9.1253146820236e-05563\n",
      "Episode Reward: 3.0\n",
      "Step 298 (41169) @ Episode 166/10000, loss: 0.00240543251857161526\n",
      "Episode Reward: 2.0\n",
      "Step 170 (41339) @ Episode 167/10000, loss: 0.00232362304814159878\n",
      "Episode Reward: 0.0\n",
      "Step 376 (41715) @ Episode 168/10000, loss: 0.00019346852786839008\n",
      "Episode Reward: 3.0\n",
      "Step 175 (41890) @ Episode 169/10000, loss: 0.00013678662071470173\n",
      "Episode Reward: 0.0\n",
      "Step 303 (42193) @ Episode 170/10000, loss: 0.00116966373752802642\n",
      "Episode Reward: 3.0\n",
      "Step 227 (42420) @ Episode 171/10000, loss: 0.00012230257561895996\n",
      "Episode Reward: 1.0\n",
      "Step 300 (42720) @ Episode 172/10000, loss: 0.00140208494849503044\n",
      "Episode Reward: 2.0\n",
      "Step 295 (43015) @ Episode 173/10000, loss: 0.00244980351999402055\n",
      "Episode Reward: 2.0\n",
      "Step 242 (43257) @ Episode 174/10000, loss: 0.00011603201710386202\n",
      "Episode Reward: 1.0\n",
      "Step 216 (43473) @ Episode 175/10000, loss: 0.00028202831163071096\n",
      "Episode Reward: 1.0\n",
      "Step 166 (43639) @ Episode 176/10000, loss: 0.00147094577550888062\n",
      "Episode Reward: 0.0\n",
      "Step 234 (43873) @ Episode 177/10000, loss: 0.00021799516980536282\n",
      "Episode Reward: 1.0\n",
      "Step 254 (44127) @ Episode 178/10000, loss: 0.00021696262410841882\n",
      "Episode Reward: 1.0\n",
      "Step 272 (44399) @ Episode 179/10000, loss: 0.00048593946848995984\n",
      "Episode Reward: 2.0\n",
      "Step 210 (44609) @ Episode 180/10000, loss: 0.00020895448687952012\n",
      "Episode Reward: 1.0\n",
      "Step 344 (44953) @ Episode 181/10000, loss: 0.00154618790838867436\n",
      "Episode Reward: 3.0\n",
      "Step 229 (45182) @ Episode 182/10000, loss: 0.00104199966881424194\n",
      "Episode Reward: 1.0\n",
      "Step 181 (45363) @ Episode 183/10000, loss: 0.00021649526024702936\n",
      "Episode Reward: 0.0\n",
      "Step 229 (45592) @ Episode 184/10000, loss: 0.00045607110951095826\n",
      "Episode Reward: 1.0\n",
      "Step 391 (45983) @ Episode 185/10000, loss: 0.00024559046141803265\n",
      "Episode Reward: 4.0\n",
      "Step 168 (46151) @ Episode 186/10000, loss: 7.842840568628162e-053\n",
      "Episode Reward: 0.0\n",
      "Step 215 (46366) @ Episode 187/10000, loss: 0.00011090129555668682\n",
      "Episode Reward: 1.0\n",
      "Step 178 (46544) @ Episode 188/10000, loss: 0.00010388453665655106\n",
      "Episode Reward: 0.0\n",
      "Step 395 (46939) @ Episode 189/10000, loss: 0.00150445010513067255\n",
      "Episode Reward: 4.0\n",
      "Step 171 (47110) @ Episode 190/10000, loss: 0.00012564193457365036\n",
      "Episode Reward: 0.0\n",
      "Step 236 (47346) @ Episode 191/10000, loss: 4.929274655296467e-058\n",
      "Episode Reward: 1.0\n",
      "Step 231 (47577) @ Episode 192/10000, loss: 0.00010270522034261376\n",
      "Episode Reward: 1.0\n",
      "Step 339 (47916) @ Episode 193/10000, loss: 6.065956404199824e-057\n",
      "Episode Reward: 3.0\n",
      "Step 312 (48228) @ Episode 194/10000, loss: 0.00014596368419006467\n",
      "Episode Reward: 2.0\n",
      "Step 172 (48400) @ Episode 195/10000, loss: 0.00111192837357521063\n",
      "Episode Reward: 0.0\n",
      "Step 295 (48695) @ Episode 196/10000, loss: 0.00023275212151929736\n",
      "Episode Reward: 2.0\n",
      "Step 236 (48931) @ Episode 197/10000, loss: 0.00046468953951261944\n",
      "Episode Reward: 2.0\n",
      "Step 245 (49176) @ Episode 198/10000, loss: 8.970996714197099e-057\n",
      "Episode Reward: 1.0\n",
      "Step 236 (49412) @ Episode 199/10000, loss: 0.00058840966084972023\n",
      "Episode Reward: 1.0\n",
      "Step 272 (49684) @ Episode 200/10000, loss: 9.175945888273418e-053\n",
      "Episode Reward: 2.0\n",
      "Step 239 (49923) @ Episode 201/10000, loss: 0.00270044803619384778\n",
      "Episode Reward: 1.0\n",
      "Step 76 (49999) @ Episode 202/10000, loss: 0.00018931982049252838\n",
      "Copied model parameters to target network.\n",
      "Step 187 (50110) @ Episode 202/10000, loss: 0.00602043792605400167\n",
      "Episode Reward: 0.0\n",
      "Step 181 (50291) @ Episode 203/10000, loss: 0.00205161585472524176\n",
      "Episode Reward: 0.0\n",
      "Step 629 (50920) @ Episode 204/10000, loss: 0.00057615933474153283\n",
      "Episode Reward: 7.0\n",
      "Step 176 (51096) @ Episode 205/10000, loss: 0.00015449651982635264\n",
      "Episode Reward: 0.0\n",
      "Step 210 (51306) @ Episode 206/10000, loss: 0.00011187351628905162\n",
      "Episode Reward: 1.0\n",
      "Step 348 (51654) @ Episode 207/10000, loss: 0.00201944354921579366\n",
      "Episode Reward: 3.0\n",
      "Step 182 (51836) @ Episode 208/10000, loss: 0.00101898657158017162\n",
      "Episode Reward: 0.0\n",
      "Step 210 (52046) @ Episode 209/10000, loss: 0.00229274434968829157\n",
      "Episode Reward: 1.0\n",
      "Step 180 (52226) @ Episode 210/10000, loss: 0.00071178202051669365\n",
      "Episode Reward: 0.0\n",
      "Step 249 (52475) @ Episode 211/10000, loss: 0.00145143189001828435\n",
      "Episode Reward: 1.0\n",
      "Step 229 (52704) @ Episode 212/10000, loss: 0.00112891045864671477\n",
      "Episode Reward: 1.0\n",
      "Step 273 (52977) @ Episode 213/10000, loss: 0.00040773395448923114\n",
      "Episode Reward: 2.0\n",
      "Step 188 (53165) @ Episode 214/10000, loss: 0.00164197036065161236\n",
      "Episode Reward: 0.0\n",
      "Step 242 (53407) @ Episode 215/10000, loss: 0.00040318505489267416\n",
      "Episode Reward: 1.0\n",
      "Step 239 (53646) @ Episode 216/10000, loss: 0.00019063887884840375\n",
      "Episode Reward: 1.0\n",
      "Step 209 (53855) @ Episode 217/10000, loss: 0.00202775024808943278\n",
      "Episode Reward: 1.0\n",
      "Step 217 (54072) @ Episode 218/10000, loss: 0.00016021520423237234\n",
      "Episode Reward: 1.0\n",
      "Step 281 (54353) @ Episode 219/10000, loss: 0.00791946239769458858\n",
      "Episode Reward: 1.0\n",
      "Step 299 (54652) @ Episode 220/10000, loss: 0.00020233701798133552\n",
      "Episode Reward: 2.0\n",
      "Step 487 (55139) @ Episode 221/10000, loss: 0.00372420670464634982\n",
      "Episode Reward: 5.0\n",
      "Step 293 (55432) @ Episode 222/10000, loss: 0.00073984381742775445\n",
      "Episode Reward: 2.0\n",
      "Step 273 (55705) @ Episode 223/10000, loss: 4.8012447223300114e-05\n",
      "Episode Reward: 2.0\n",
      "Step 180 (55885) @ Episode 224/10000, loss: 0.00030095654074102642\n",
      "Episode Reward: 0.0\n",
      "Step 341 (56226) @ Episode 225/10000, loss: 0.00054545281454920772\n",
      "Episode Reward: 3.0\n",
      "Step 249 (56475) @ Episode 226/10000, loss: 0.00197277124971151357\n",
      "Episode Reward: 1.0\n",
      "Step 285 (56760) @ Episode 227/10000, loss: 0.00011874664778588343\n",
      "Episode Reward: 3.0\n",
      "Step 194 (56954) @ Episode 228/10000, loss: 0.00241568125784397137\n",
      "Episode Reward: 0.0\n",
      "Step 192 (57146) @ Episode 229/10000, loss: 0.00014475078205578034\n",
      "Episode Reward: 0.0\n",
      "Step 235 (57381) @ Episode 230/10000, loss: 0.00024897500406950715\n",
      "Episode Reward: 1.0\n",
      "Step 227 (57608) @ Episode 231/10000, loss: 0.00010304584429832175\n",
      "Episode Reward: 1.0\n",
      "Step 339 (57947) @ Episode 232/10000, loss: 0.00012716940545942634\n",
      "Episode Reward: 3.0\n",
      "Step 332 (58279) @ Episode 233/10000, loss: 0.00022853094560559847\n",
      "Episode Reward: 3.0\n",
      "Step 175 (58454) @ Episode 234/10000, loss: 0.00020098482491448522\n",
      "Episode Reward: 0.0\n",
      "Step 187 (58641) @ Episode 235/10000, loss: 6.124942592578009e-052\n",
      "Episode Reward: 0.0\n",
      "Step 270 (58911) @ Episode 236/10000, loss: 0.00025979417841881514\n",
      "Episode Reward: 2.0\n",
      "Step 263 (59174) @ Episode 237/10000, loss: 0.00056162808323279026\n",
      "Episode Reward: 2.0\n",
      "Step 220 (59394) @ Episode 238/10000, loss: 0.00023009526194073268\n",
      "Episode Reward: 1.0\n",
      "Step 254 (59648) @ Episode 239/10000, loss: 0.00048632471589371567\n",
      "Episode Reward: 1.0\n",
      "Step 234 (59882) @ Episode 240/10000, loss: 0.00101906375493854288\n",
      "Episode Reward: 1.0\n",
      "Step 117 (59999) @ Episode 241/10000, loss: 0.00018933601677417755\n",
      "Copied model parameters to target network.\n",
      "Step 169 (60051) @ Episode 241/10000, loss: 0.00014866393757984042\n",
      "Episode Reward: 0.0\n",
      "Step 320 (60371) @ Episode 242/10000, loss: 0.00480022840201854773\n",
      "Episode Reward: 3.0\n",
      "Step 267 (60638) @ Episode 243/10000, loss: 0.00201372592709958557\n",
      "Episode Reward: 2.0\n",
      "Step 226 (60864) @ Episode 244/10000, loss: 0.00232895347289741043\n",
      "Episode Reward: 1.0\n",
      "Step 267 (61131) @ Episode 245/10000, loss: 7.35982321202755e-0569\n",
      "Episode Reward: 2.0\n",
      "Step 175 (61306) @ Episode 246/10000, loss: 5.840917583554983e-058\n",
      "Episode Reward: 0.0\n",
      "Step 183 (61489) @ Episode 247/10000, loss: 6.986450171098113e-054\n",
      "Episode Reward: 0.0\n",
      "Step 391 (61880) @ Episode 248/10000, loss: 0.00351362815126776753\n",
      "Episode Reward: 3.0\n",
      "Step 180 (62060) @ Episode 249/10000, loss: 0.00044129596790298825\n",
      "Episode Reward: 0.0\n",
      "Step 436 (62496) @ Episode 250/10000, loss: 0.00030404928838834167\n",
      "Episode Reward: 4.0\n",
      "Step 209 (62705) @ Episode 251/10000, loss: 9.434258390683681e-053\n",
      "Episode Reward: 1.0\n",
      "Step 212 (62917) @ Episode 252/10000, loss: 0.00064045662293210633\n",
      "Episode Reward: 1.0\n",
      "Step 246 (63163) @ Episode 253/10000, loss: 0.00271101808175444656\n",
      "Episode Reward: 1.0\n",
      "Step 370 (63533) @ Episode 254/10000, loss: 7.394656131509691e-052\n",
      "Episode Reward: 3.0\n",
      "Step 245 (63778) @ Episode 255/10000, loss: 0.00038330262759700427\n",
      "Episode Reward: 1.0\n",
      "Step 173 (63951) @ Episode 256/10000, loss: 0.00146440765820443633\n",
      "Episode Reward: 0.0\n",
      "Step 362 (64313) @ Episode 257/10000, loss: 0.00038209784543141722\n",
      "Episode Reward: 3.0\n",
      "Step 277 (64590) @ Episode 258/10000, loss: 0.00412014545872807586\n",
      "Episode Reward: 2.0\n",
      "Step 248 (64838) @ Episode 259/10000, loss: 0.00029254425317049026\n",
      "Episode Reward: 1.0\n",
      "Step 236 (65074) @ Episode 260/10000, loss: 0.00065957574406638742\n",
      "Episode Reward: 1.0\n",
      "Step 231 (65305) @ Episode 261/10000, loss: 0.00036454549990594387\n",
      "Episode Reward: 1.0\n",
      "Step 215 (65520) @ Episode 262/10000, loss: 6.024444519425742e-054\n",
      "Episode Reward: 1.0\n",
      "Step 363 (65883) @ Episode 263/10000, loss: 0.00011416655615903437\n",
      "Episode Reward: 3.0\n",
      "Step 220 (66103) @ Episode 264/10000, loss: 7.069940329529345e-056\n",
      "Episode Reward: 1.0\n",
      "Step 304 (66407) @ Episode 265/10000, loss: 0.00026624643942341214\n",
      "Episode Reward: 3.0\n",
      "Step 217 (66624) @ Episode 266/10000, loss: 0.00049250212032347927\n",
      "Episode Reward: 1.0\n",
      "Step 213 (66837) @ Episode 267/10000, loss: 0.00567192165181040868\n",
      "Episode Reward: 1.0\n",
      "Step 230 (67067) @ Episode 268/10000, loss: 0.00252688443288207057\n",
      "Episode Reward: 1.0\n",
      "Step 239 (67306) @ Episode 269/10000, loss: 0.00229589710943400865\n",
      "Episode Reward: 1.0\n",
      "Step 269 (67575) @ Episode 270/10000, loss: 0.00116197683382779364\n",
      "Episode Reward: 2.0\n",
      "Step 177 (67752) @ Episode 271/10000, loss: 0.00017903874686453491\n",
      "Episode Reward: 0.0\n",
      "Step 245 (67997) @ Episode 272/10000, loss: 0.00016258830146398395\n",
      "Episode Reward: 1.0\n",
      "Step 404 (68401) @ Episode 273/10000, loss: 0.00016320068971253932\n",
      "Episode Reward: 4.0\n",
      "Step 351 (68752) @ Episode 274/10000, loss: 0.00063263636548072127\n",
      "Episode Reward: 3.0\n",
      "Step 168 (68920) @ Episode 275/10000, loss: 0.00130155682563781744\n",
      "Episode Reward: 0.0\n",
      "Step 304 (69224) @ Episode 276/10000, loss: 0.00049791624769568443\n",
      "Episode Reward: 2.0\n",
      "Step 163 (69387) @ Episode 277/10000, loss: 0.00011920550605282187\n",
      "Episode Reward: 0.0\n",
      "Step 259 (69646) @ Episode 278/10000, loss: 0.00157912494614720345\n",
      "Episode Reward: 1.0\n",
      "Step 334 (69980) @ Episode 279/10000, loss: 9.509087249170989e-054\n",
      "Episode Reward: 3.0\n",
      "Step 19 (69999) @ Episode 280/10000, loss: 2.724469595705159e-056\n",
      "Copied model parameters to target network.\n",
      "Step 242 (70222) @ Episode 280/10000, loss: 0.00173938903026282796\n",
      "Episode Reward: 1.0\n",
      "Step 178 (70400) @ Episode 281/10000, loss: 7.583726255688816e-053\n",
      "Episode Reward: 0.0\n",
      "Step 174 (70574) @ Episode 282/10000, loss: 0.00420195981860160814\n",
      "Episode Reward: 0.0\n",
      "Step 177 (70751) @ Episode 283/10000, loss: 0.00011058403470087796\n",
      "Episode Reward: 0.0\n",
      "Step 266 (71017) @ Episode 284/10000, loss: 0.00209655845537781752\n",
      "Episode Reward: 2.0\n",
      "Step 280 (71297) @ Episode 285/10000, loss: 0.00025847056531347334\n",
      "Episode Reward: 2.0\n",
      "Step 247 (71544) @ Episode 286/10000, loss: 0.00025353461387567222\n",
      "Episode Reward: 2.0\n",
      "Step 355 (71899) @ Episode 287/10000, loss: 0.00129924924112856394\n",
      "Episode Reward: 3.0\n",
      "Step 243 (72142) @ Episode 288/10000, loss: 0.00017139094416052103\n",
      "Episode Reward: 1.0\n",
      "Step 173 (72315) @ Episode 289/10000, loss: 0.00014928510063327856\n",
      "Episode Reward: 0.0\n",
      "Step 405 (72720) @ Episode 290/10000, loss: 8.989574416773394e-054\n",
      "Episode Reward: 4.0\n",
      "Step 229 (72949) @ Episode 291/10000, loss: 0.00351274176500737677\n",
      "Episode Reward: 1.0\n",
      "Step 516 (73465) @ Episode 292/10000, loss: 0.00018053181702271104\n",
      "Episode Reward: 6.0\n",
      "Step 236 (73701) @ Episode 293/10000, loss: 0.00021705601830035448\n",
      "Episode Reward: 1.0\n",
      "Step 231 (73932) @ Episode 294/10000, loss: 0.00048423744738101964\n",
      "Episode Reward: 1.0\n",
      "Step 305 (74237) @ Episode 295/10000, loss: 0.00189290475100278857\n",
      "Episode Reward: 2.0\n",
      "Step 402 (74639) @ Episode 296/10000, loss: 0.00077729875920340426\n",
      "Episode Reward: 4.0\n",
      "Step 167 (74806) @ Episode 297/10000, loss: 0.00170472089666873225\n",
      "Episode Reward: 0.0\n",
      "Step 433 (75239) @ Episode 298/10000, loss: 0.00128291768487542878\n",
      "Episode Reward: 4.0\n",
      "Step 266 (75505) @ Episode 299/10000, loss: 0.00534383254125714305\n",
      "Episode Reward: 2.0\n",
      "Step 216 (75721) @ Episode 300/10000, loss: 0.00252108415588736533\n",
      "Episode Reward: 1.0\n",
      "Step 205 (75926) @ Episode 301/10000, loss: 0.00011671060929074883\n",
      "Episode Reward: 1.0\n",
      "Step 177 (76103) @ Episode 302/10000, loss: 0.00037384423194453126\n",
      "Episode Reward: 0.0\n",
      "Step 189 (76292) @ Episode 303/10000, loss: 8.016002539079636e-054\n",
      "Episode Reward: 0.0\n",
      "Step 244 (76536) @ Episode 304/10000, loss: 0.00021342316176742315\n",
      "Episode Reward: 1.0\n",
      "Step 290 (76826) @ Episode 305/10000, loss: 0.00227830721996724654\n",
      "Episode Reward: 2.0\n",
      "Step 171 (76997) @ Episode 306/10000, loss: 0.00050786294741556056\n",
      "Episode Reward: 0.0\n",
      "Step 178 (77175) @ Episode 307/10000, loss: 0.00024776053032837816\n",
      "Episode Reward: 0.0\n",
      "Step 170 (77345) @ Episode 308/10000, loss: 0.00021574064157903194\n",
      "Episode Reward: 0.0\n",
      "Step 232 (77577) @ Episode 309/10000, loss: 8.915219223126769e-051\n",
      "Episode Reward: 1.0\n",
      "Step 244 (77821) @ Episode 310/10000, loss: 0.00028277066303417087\n",
      "Episode Reward: 1.0\n",
      "Step 168 (77989) @ Episode 311/10000, loss: 8.492040069540963e-056\n",
      "Episode Reward: 0.0\n",
      "Step 432 (78421) @ Episode 312/10000, loss: 0.00100093195214867685\n",
      "Episode Reward: 4.0\n",
      "Step 190 (78611) @ Episode 313/10000, loss: 0.00022606413403991614\n",
      "Episode Reward: 0.0\n",
      "Step 211 (78822) @ Episode 314/10000, loss: 0.00141800055280327815\n",
      "Episode Reward: 1.0\n",
      "Step 167 (78989) @ Episode 315/10000, loss: 0.00023564870934933424\n",
      "Episode Reward: 0.0\n",
      "Step 339 (79328) @ Episode 316/10000, loss: 0.00016149326984304935\n",
      "Episode Reward: 3.0\n",
      "Step 179 (79507) @ Episode 317/10000, loss: 0.00151206902228295837\n",
      "Episode Reward: 0.0\n",
      "Step 211 (79718) @ Episode 318/10000, loss: 6.79153308738023e-0532\n",
      "Episode Reward: 1.0\n",
      "Step 250 (79968) @ Episode 319/10000, loss: 0.00127763568889349775\n",
      "Episode Reward: 1.0\n",
      "Step 31 (79999) @ Episode 320/10000, loss: 0.00102480861824005845\n",
      "Copied model parameters to target network.\n",
      "Step 355 (80323) @ Episode 320/10000, loss: 0.00021229087724350393\n",
      "Episode Reward: 3.0\n",
      "Step 299 (80622) @ Episode 321/10000, loss: 8.690984395798296e-058\n",
      "Episode Reward: 2.0\n",
      "Step 164 (80786) @ Episode 322/10000, loss: 0.00238327682018280037\n",
      "Episode Reward: 0.0\n",
      "Step 189 (80975) @ Episode 323/10000, loss: 0.00293292244896292736\n",
      "Episode Reward: 0.0\n",
      "Step 359 (81334) @ Episode 324/10000, loss: 0.00235796114429831527\n",
      "Episode Reward: 3.0\n",
      "Step 179 (81513) @ Episode 325/10000, loss: 0.00705613987520337125\n",
      "Episode Reward: 0.0\n",
      "Step 228 (81741) @ Episode 326/10000, loss: 0.00141980964690446854\n",
      "Episode Reward: 1.0\n",
      "Step 283 (82024) @ Episode 327/10000, loss: 0.00147238513454794887\n",
      "Episode Reward: 2.0\n",
      "Step 246 (82270) @ Episode 328/10000, loss: 0.00039856077637523418\n",
      "Episode Reward: 1.0\n",
      "Step 266 (82536) @ Episode 329/10000, loss: 0.00200611609034240255\n",
      "Episode Reward: 2.0\n",
      "Step 185 (82721) @ Episode 330/10000, loss: 5.5487253121100366e-05\n",
      "Episode Reward: 0.0\n",
      "Step 311 (83032) @ Episode 331/10000, loss: 0.00109441787935793413\n",
      "Episode Reward: 3.0\n",
      "Step 210 (83242) @ Episode 332/10000, loss: 4.36898299085442e-0557\n",
      "Episode Reward: 1.0\n",
      "Step 243 (83485) @ Episode 333/10000, loss: 0.00026697522844187915\n",
      "Episode Reward: 1.0\n",
      "Step 204 (83689) @ Episode 334/10000, loss: 0.00472825346514582647\n",
      "Episode Reward: 0.0\n",
      "Step 176 (83865) @ Episode 335/10000, loss: 0.00015495429397560656\n",
      "Episode Reward: 0.0\n",
      "Step 308 (84173) @ Episode 336/10000, loss: 0.00207519507966935633\n",
      "Episode Reward: 3.0\n",
      "Step 278 (84451) @ Episode 337/10000, loss: 0.00112677889410406355\n",
      "Episode Reward: 2.0\n",
      "Step 185 (84636) @ Episode 338/10000, loss: 0.00034662382677197456\n",
      "Episode Reward: 0.0\n",
      "Step 277 (84913) @ Episode 339/10000, loss: 0.00026763428468257196\n",
      "Episode Reward: 2.0\n",
      "Step 366 (85279) @ Episode 340/10000, loss: 0.00088070752099156386\n",
      "Episode Reward: 4.0\n",
      "Step 343 (85622) @ Episode 341/10000, loss: 0.00014300338807515897\n",
      "Episode Reward: 3.0\n",
      "Step 240 (85862) @ Episode 342/10000, loss: 0.00041513139149174094\n",
      "Episode Reward: 1.0\n",
      "Step 281 (86143) @ Episode 343/10000, loss: 0.00107024249155074365\n",
      "Episode Reward: 2.0\n",
      "Step 171 (86314) @ Episode 344/10000, loss: 0.00040414204704575244\n",
      "Episode Reward: 0.0\n",
      "Step 184 (86498) @ Episode 345/10000, loss: 0.00130237848497927192\n",
      "Episode Reward: 0.0\n",
      "Step 209 (86707) @ Episode 346/10000, loss: 0.00128618150483816865\n",
      "Episode Reward: 1.0\n",
      "Step 224 (86931) @ Episode 347/10000, loss: 9.000521095003933e-056\n",
      "Episode Reward: 1.0\n",
      "Step 160 (87091) @ Episode 348/10000, loss: 0.00210276851430535346\n",
      "Episode Reward: 0.0\n",
      "Step 34 (87125) @ Episode 349/10000, loss: 0.00209739501588046557"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "#                                     replay_memory_init_size=50000,\n",
    "                                    replay_memory_init_size=500,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
